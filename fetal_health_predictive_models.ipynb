{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f57a0cd-d061-4700-a553-5f8e6100a971",
   "metadata": {},
   "source": [
    "# Predictive Models\n",
    "\n",
    "In this notebook we will use several predictive models to try to classify the input data into correct fetal health group. Compare models to see which one is better and select the best model in use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879858ca-4c1e-42da-909f-5019c66ee11b",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928897af-c0e9-488f-836b-748e738efb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76571bd9-4fba-4c01-8178-60e1e1714804",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac850c0-cb31-4ce8-9cb3-323cb389574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fetal_health.csv')\n",
    "X = data.drop(columns=['fetal_health'])\n",
    "y = data['fetal_health']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8182ed-5dc2-48a9-9387-9f2952b2128c",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "We will evaluate the model base on the accuracy, precision, recall, and f1 score.\n",
    "Generally, the model achieve accuracy of 0.88 which is very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8298dd3d-ffa7-44c1-9466-c499d4b2c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Evaluation Metrics:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.87\n",
      "Recall: 0.88\n",
      "F1 Score: 0.87\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.97      0.94       332\n",
      "         2.0       0.60      0.46      0.52        59\n",
      "         3.0       0.96      0.66      0.78        35\n",
      "\n",
      "    accuracy                           0.88       426\n",
      "   macro avg       0.82      0.70      0.75       426\n",
      "weighted avg       0.87      0.88      0.87       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(solver='liblinear', max_iter=500)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "lr_accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Logistic Regression Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {lr_accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96382c36-8957-470d-92e4-1d5e18e656e8",
   "metadata": {},
   "source": [
    "## Random Forest Model\n",
    "\n",
    "Same with Logistic Regression model, we evaluate the model base on the same metrics. The Random Forest seem to perform better with 0.92 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a6e46c-259e-4367-8e5a-cce73f8a01a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Evaluation Metrics:\n",
      "Accuracy: 0.92\n",
      "Precision: 0.92\n",
      "Recall: 0.92\n",
      "F1 Score: 0.92\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.98      0.96       332\n",
      "         2.0       0.83      0.68      0.75        59\n",
      "         3.0       0.85      0.83      0.84        35\n",
      "\n",
      "    accuracy                           0.92       426\n",
      "   macro avg       0.88      0.83      0.85       426\n",
      "weighted avg       0.92      0.92      0.92       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_model = RandomForestClassifier(random_state=42, n_estimators=100)  # You can adjust n_estimators for optimization\n",
    "\n",
    "# Train the Random Forest model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the Random Forest model\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {rf_accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023ed26-6e06-4e24-aa10-3bb82e69c01d",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "\n",
    "Same with Logistic Regression model, we evaluate the model base on the same metrics. The Decision Tree seem to perform better than Logistic Regression model with 0.90 accuracy, but slightly worse than Random Forest by 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f0af86-3ca2-4901-9969-0d5ab7dd9470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Evaluation Metrics:\n",
      "Accuracy: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1 Score: 0.90\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.95      0.95       332\n",
      "         2.0       0.73      0.64      0.68        59\n",
      "         3.0       0.81      0.83      0.82        35\n",
      "\n",
      "    accuracy                           0.90       426\n",
      "   macro avg       0.82      0.81      0.82       426\n",
      "weighted avg       0.90      0.90      0.90       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decision_tree_model.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Decision Tree Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {dt_accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc2190-e414-459c-a15b-3b41e689c329",
   "metadata": {},
   "source": [
    "## Conclusion models evaluation and which one to choose\n",
    "\n",
    "We can see that random forest perform better than other models not in just accuracy but also most of other metrics.\n",
    "Conclusion: For this three models, we will choose Random Forest Model to use in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78287ed1-1d8c-44b6-be8d-e96694c5b20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:\n",
      "Logistic Regression:  0.8755868544600939\n",
      "Random Forest:  0.9248826291079812\n",
      "Decision Tree:  0.9014084507042254\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Accuracy:\")\n",
    "print(\"Logistic Regression: \", lr_accuracy)\n",
    "print(\"Random Forest: \", rf_accuracy)\n",
    "print(\"Decision Tree: \", dt_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a8d2f-ff3a-4637-b645-7618ab13e52f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
